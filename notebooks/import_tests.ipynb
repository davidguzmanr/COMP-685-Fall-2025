{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google earth engine should work if installed, but it needs authentication\n",
    "# making datasets from it with a computer with internet may be preferable to using it while on a compute node\n",
    "# import ee\n",
    "\n",
    "# # Trigger the authentication flow.\n",
    "# ee.Authenticate()\n",
    "\n",
    "# # Initialize the library.\n",
    "# ee.Initialize(project='my-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygbif import species as species\n",
    "from pygbif import occurrences as occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splist = ['Cyanocitta stelleri', 'Junco hyemalis', 'Aix sponsa',\n",
    "  'Ursus americanus', 'Pinus conorta', 'Poa annuus']\n",
    "keys = [ species.name_backbone(x)['usageKey'] for x in splist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [ occ.search(taxonKey = x, limit=0)['count'] for x in keys ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Junco hyemalis', 14827936),\n",
       " ('Aix sponsa', 6498428),\n",
       " ('Cyanocitta stelleri', 2756192),\n",
       " ('Poa annuus', 741730),\n",
       " ('Pinus conorta', 105934),\n",
       " ('Ursus americanus', 53946)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dict(zip(splist, out))\n",
    "sorted(x.items(), key=lambda z:z[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SatCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone satclip into a folder, use your own username\n",
    "satclip_path = \"/home/mila/g/<username>/scratch/satclip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(satclip_path)\n",
    "sys.path.insert(0, \"/home/mila/g/<username>/scratch/satclip/satclip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/scratch/g/gregory.bell/COMP-685-Fall-2025/.venv/lib/python3.11/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from satclip.model import SatCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satclip.location_encoder import LocationEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vision transformer\n"
     ]
    }
   ],
   "source": [
    "model = SatCLIP(\n",
    "    embed_dim=512,\n",
    "    image_resolution=224, in_channels=13, vision_layers=4, vision_width=768, vision_patch_size=32, # Image encoder\n",
    "    le_type='sphericalharmonics', pe_type='siren', legendre_polys=10, frequency_num=16, max_radius=360, min_radius=1, harmonics_calculation='analytic'  # Location encoder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = torch.randn(32, 13, 224, 224) # Represents a batch of 32 images\n",
    "loc_batch = torch.randn(32, 2) # Represents the corresponding 32 locations (lon/lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits_per_image, logits_per_coord = model(img_batch, loc_batch)\n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INaturalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyinaturalist import get_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations\n",
      "User-Agent: python-requests/2.32.5 pyinaturalist/0.20.2\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observations = get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'total_results'\u001b[0m, \u001b[32m'page'\u001b[0m, \u001b[32m'per_page'\u001b[0m, \u001b[32m'results'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m312784440\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations['total_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account api key is needed, may be best to load dataset outside computer node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone in another folder and install in the uv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/scratch/g/gregory.bell/COMP-685-Fall-2025/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1280])\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers>=4.53 torch numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from transformers import TimesFmModelForPrediction\n",
    "\n",
    "# 1) Load a TimesFM checkpoint (univariate)\n",
    "model = TimesFmModelForPrediction.from_pretrained(\n",
    "    \"google/timesfm-2.0-500m-pytorch\",  # or a newer 2.5 checkpoint if/when available\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 2) Example batch of variable-length time series (replace with your data)\n",
    "series = [\n",
    "    np.sin(np.linspace(0, 20, 100)),   # length 100\n",
    "    np.sin(np.linspace(0, 20, 240)),   # length 240\n",
    "    np.sin(np.linspace(0, 20, 512)),   # length 512\n",
    "    np.sin(np.linspace(0, 20, 1024)),   # length 1024\n",
    "]\n",
    "\n",
    "# Convert to tensors on the model's device\n",
    "past_values = [torch.tensor(x, dtype=torch.bfloat16, device=model.device) for x in series]\n",
    "\n",
    "# TimesFM uses a small frequency embedding; if you don't know it, you can set zeros.\n",
    "# Shape: (batch,)\n",
    "freq = torch.zeros(len(past_values), dtype=torch.long, device=model.device)\n",
    "\n",
    "# 3) Forward pass requesting hidden states\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "        past_values=past_values,\n",
    "        freq=freq,\n",
    "        output_hidden_states=True,   # <â€” important\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "# out.last_hidden_state: (batch, seq_len, hidden_size) after internal padding\n",
    "# We'll mean-pool over the *valid* timesteps for each series, then L2-normalize.\n",
    "\n",
    "embeddings = []\n",
    "for i, ts in enumerate(past_values):\n",
    "    L = ts.shape[0]  # original (unpadded) length\n",
    "    # slice to valid region, mean-pool over time\n",
    "    vec = out.last_hidden_state[i, :L, :].float().mean(dim=0)\n",
    "    embeddings.append(vec)\n",
    "\n",
    "embeddings = torch.stack(embeddings, dim=0)           # (batch, hidden_size)\n",
    "embeddings = normalize(embeddings, p=2, dim=1)        # cosine-friendly\n",
    "\n",
    "print(embeddings.shape)   # e.g., torch.Size([4, 1280]) for the 500M checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-series embedding shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# 1) Load a Chronos (T5) checkpoint\n",
    "pipe = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",           # try base/large for bigger models\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "# 2) Example batch: variable-length univariate series\n",
    "rng = np.random.default_rng(0)\n",
    "series = [\n",
    "    np.sin(np.linspace(0, 8*np.pi, 120)) + 0.1 * rng.standard_normal(120),\n",
    "    np.sin(np.linspace(0, 8*np.pi, 200)) + 0.1 * rng.standard_normal(200),\n",
    "    np.sin(np.linspace(0, 8*np.pi, 350)) + 0.1 * rng.standard_normal(350),\n",
    "]\n",
    "\n",
    "# Convert to a list of 1D tensors (no manual padding needed)\n",
    "batch = [torch.tensor(x, dtype=torch.float32) for x in series]\n",
    "lengths = [len(x) for x in batch]\n",
    "\n",
    "# 3) Extract encoder embeddings for the whole batch\n",
    "#    emb has shape (batch, seq_len, hidden) with internal padding handled for you.\n",
    "emb, tok_state = pipe.embed(batch)\n",
    "\n",
    "# 4) Pool to a single fixed-size vector per series (mask out padding)\n",
    "#    We'll mean-pool over the valid (unpadded) timesteps, then L2-normalize.\n",
    "pooled = []\n",
    "for i, L in enumerate(lengths):\n",
    "    vec = emb[i, :L, :].float().mean(dim=0)   # (hidden,)\n",
    "    pooled.append(vec)\n",
    "\n",
    "embeddings = torch.stack(pooled, dim=0)       # (batch, hidden)\n",
    "embeddings = normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Per-series embedding shape:\", embeddings.shape)   # e.g., torch.Size([3, hidden])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
